{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0a46f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce510ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "RANDOM_SEED = 10\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "# 경로 설정\n",
    "ROOT_PATH = 'C:/Users/crid2/'\n",
    "\n",
    "# DATA_DIR = os.path.join(ROOT_PATH, 'data')\n",
    "DATA_DIR = 'kepcodata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ec82988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c077d035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>누적전력량</th>\n",
       "      <th>유효전력평균</th>\n",
       "      <th>무효전력평균</th>\n",
       "      <th>주파수</th>\n",
       "      <th>전류평균</th>\n",
       "      <th>상전압평균</th>\n",
       "      <th>선간전압평균</th>\n",
       "      <th>온도</th>\n",
       "      <th>R상유효전력</th>\n",
       "      <th>...</th>\n",
       "      <th>S상전압</th>\n",
       "      <th>S상선간전압</th>\n",
       "      <th>T상유효전력</th>\n",
       "      <th>T상무효전력</th>\n",
       "      <th>T상전류</th>\n",
       "      <th>T상전압</th>\n",
       "      <th>T상선간전압</th>\n",
       "      <th>label_역률평균</th>\n",
       "      <th>label_전류고조파평균</th>\n",
       "      <th>label_전압고조파평균</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2248930.5</td>\n",
       "      <td>28963.0</td>\n",
       "      <td>20237.0</td>\n",
       "      <td>59.854076</td>\n",
       "      <td>45.197918</td>\n",
       "      <td>259.916656</td>\n",
       "      <td>449.916656</td>\n",
       "      <td>47.500</td>\n",
       "      <td>8663.0</td>\n",
       "      <td>...</td>\n",
       "      <td>260.75</td>\n",
       "      <td>452.75</td>\n",
       "      <td>10417.0</td>\n",
       "      <td>7304.0</td>\n",
       "      <td>48.71875</td>\n",
       "      <td>260.75</td>\n",
       "      <td>448.75</td>\n",
       "      <td>정상</td>\n",
       "      <td>경고</td>\n",
       "      <td>주의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.68750</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>경고</td>\n",
       "      <td>정상</td>\n",
       "      <td>정상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5375707.0</td>\n",
       "      <td>35244.0</td>\n",
       "      <td>19826.0</td>\n",
       "      <td>59.975650</td>\n",
       "      <td>107.385414</td>\n",
       "      <td>125.416664</td>\n",
       "      <td>216.750000</td>\n",
       "      <td>18.750</td>\n",
       "      <td>11988.0</td>\n",
       "      <td>...</td>\n",
       "      <td>125.25</td>\n",
       "      <td>216.25</td>\n",
       "      <td>12236.0</td>\n",
       "      <td>6170.0</td>\n",
       "      <td>109.56250</td>\n",
       "      <td>125.00</td>\n",
       "      <td>216.25</td>\n",
       "      <td>정상</td>\n",
       "      <td>정상</td>\n",
       "      <td>주의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17781200.0</td>\n",
       "      <td>77056.0</td>\n",
       "      <td>39520.0</td>\n",
       "      <td>59.863000</td>\n",
       "      <td>244.854000</td>\n",
       "      <td>118.083000</td>\n",
       "      <td>205.333000</td>\n",
       "      <td>23.125</td>\n",
       "      <td>25796.0</td>\n",
       "      <td>...</td>\n",
       "      <td>118.75</td>\n",
       "      <td>118.75</td>\n",
       "      <td>24992.0</td>\n",
       "      <td>13704.0</td>\n",
       "      <td>242.18800</td>\n",
       "      <td>118.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>정상</td>\n",
       "      <td>정상</td>\n",
       "      <td>경고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10143988.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.798140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.750000</td>\n",
       "      <td>231.500000</td>\n",
       "      <td>26.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>134.50</td>\n",
       "      <td>231.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>133.00</td>\n",
       "      <td>230.25</td>\n",
       "      <td>경고</td>\n",
       "      <td>정상</td>\n",
       "      <td>정상</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       누적전력량   유효전력평균   무효전력평균        주파수        전류평균       상전압평균  \\\n",
       "0      0   2248930.5  28963.0  20237.0  59.854076   45.197918  259.916656   \n",
       "1      1         0.0      0.0      0.0   0.000000  101.312500    0.000000   \n",
       "2      2   5375707.0  35244.0  19826.0  59.975650  107.385414  125.416664   \n",
       "3      3  17781200.0  77056.0  39520.0  59.863000  244.854000  118.083000   \n",
       "4      4  10143988.0      0.0      0.0  59.798140    0.000000  133.750000   \n",
       "\n",
       "       선간전압평균      온도   R상유효전력  ...    S상전압  S상선간전압   T상유효전력   T상무효전력  \\\n",
       "0  449.916656  47.500   8663.0  ...  260.75  452.75  10417.0   7304.0   \n",
       "1    0.000000  24.375      0.0  ...    0.00    0.00      0.0      0.0   \n",
       "2  216.750000  18.750  11988.0  ...  125.25  216.25  12236.0   6170.0   \n",
       "3  205.333000  23.125  25796.0  ...  118.75  118.75  24992.0  13704.0   \n",
       "4  231.500000  26.875      0.0  ...  134.50  231.50      0.0      0.0   \n",
       "\n",
       "        T상전류    T상전압  T상선간전압  label_역률평균  label_전류고조파평균  label_전압고조파평균  \n",
       "0   48.71875  260.75  448.75          정상             경고             주의  \n",
       "1  100.68750    0.00    0.00          경고             정상             정상  \n",
       "2  109.56250  125.00  216.25          정상             정상             주의  \n",
       "3  242.18800  118.00  118.00          정상             정상             경고  \n",
       "4    0.00000  133.00  230.25          경고             정상             정상  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eda9cc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_전압고조파평균</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>경고</th>\n",
       "      <td>735155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>정상</th>\n",
       "      <td>994194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>주의</th>\n",
       "      <td>691216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index\n",
       "label_전압고조파평균        \n",
       "경고             735155\n",
       "정상             994194\n",
       "주의             691216"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 데이터의 감정 분류 분포\n",
    "df[['index','label_전압고조파평균']].groupby('label_전압고조파평균').count().rename(columns={'':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a8e8c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로드 파라미터\n",
    "BATCH_SIZE = 2048\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "444d48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b6d09277",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class CustomDataset(Dataset) : \n",
    "    #             \"\"\" 필수 함수 : \n",
    "    #             - __init__ : 초기화\n",
    "    #             - __len__ : 데이터셋(input)의 길이 반환\n",
    "    #             - __getitem__ : 데이터셋을 인덱스로 불러옴\n",
    "    #         \"\"\"\n",
    "\n",
    "    #         \"\"\"CustomDataset 클래스 정의\n",
    "\n",
    "    #         args:\n",
    "    #             data_dir (str)\n",
    "    #             mode (str)\n",
    "    #         \"\"\"\n",
    "        def __init__(self, data_dir, mode='train') : \n",
    "            self.mode = mode\n",
    "            self.data_dir = data_dir\n",
    "\n",
    "            # 테스트 모드 디코딩에 사용할 레이블 딕셔너리 \n",
    "            self.label_dic = {0:'정상' , 1:'주의' , 2:'경고'}\n",
    "\n",
    "            print('Loading ' + self.mode + ' dataset..')\n",
    "\n",
    "            # 데이터 경로가 존재하는지 확인\n",
    "            if not os.path.isdir(self.data_dir) : \n",
    "                print(f'!!!, Cannot find {self.data_dir}....!!!')\n",
    "                sys.exit()\n",
    "\n",
    "            # csv파일로드 \n",
    "            # val모드에서 train 데이터를 가져옵니다..? \n",
    "\n",
    "            if self.mode =='val' :\n",
    "                self.data_path = os.path.join(self.data_dir, 'train.csv')\n",
    "            else : \n",
    "                self.data_path = os.path.join(self.data_dir, f'{mode}.csv')\n",
    "\n",
    "            df = pd.read_csv(self.data_path)\n",
    "\n",
    "\n",
    "            # 학습/검증 데이터 구역선택 , 학습 데이터 비율에 따라 TRAIN_RATIO 변경\n",
    "            TRAIN_RATIO = 0.9\n",
    "            if self.mode == 'train':\n",
    "                df = df[:int(len(df)*TRAIN_RATIO)]\n",
    "\n",
    "            elif self.mode == 'val':\n",
    "                df = df[int(len(df)*TRAIN_RATIO)+1:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "            if self.mode != 'test': \n",
    "                features = df.iloc[: , 1:-3].copy().fillna(0)\n",
    "                targets = df.iloc[:  -3:].copy().fillna(0)\n",
    "\n",
    "            else : \n",
    "                features = df.iloc[:, 1:].copy().fillna(0)\n",
    "\n",
    "            feature_scaler = MinMaxScaler()\n",
    "            feature_scaler_path = 'feature_scaler.pkl'\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                feature_scaler.fit(features)\n",
    "                joblib.dump(feature_scaler, feature_scaler_path)\n",
    "            else:\n",
    "                feature_scaler = joblib.load(feature_scaler_path)\n",
    "\n",
    "            self.features = feature_scaler.transform(features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # target encoding : \n",
    "            if self.mode != 'test' : \n",
    "                self.targets = targets.replace({\n",
    "                    '정상' : 0,\n",
    "                    '주의' : 1,\n",
    "                    '경고' : 2\n",
    "\n",
    "                })\n",
    "\n",
    "        def label_decoder(self, labels) : \n",
    "            try : \n",
    "                labels = list(mp(lambda x : self.label_dic[x] , labels))\n",
    "                return labels\n",
    "            except : \n",
    "                assert 'Invalid intent'\n",
    "\n",
    "\n",
    "        def __len__(self) : \n",
    "            return self.features.shape[0]\n",
    "\n",
    "\n",
    "        # 23개의 피쳐들과 test 모드일 때를 제외하고 target 세가지 를 리턴 \n",
    "\n",
    "        def __getitem__(self, idx:int) : \n",
    "\n",
    "            feature = self.features[idx]  \n",
    "            feature = torch.tensor(feature, dtype=torch.float)\n",
    "\n",
    "            if self.mode != 'test' : \n",
    "                target_1 = int(self.targets.iloc[idx, 0])\n",
    "                target_2 = int(self.targets.iloc[idx, 1])\n",
    "                target_3 = int(self.targets.iloc[idx, 2])\n",
    "\n",
    "            else : \n",
    "                return feature\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28a9420c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "Loading val dataset..\n"
     ]
    }
   ],
   "source": [
    "# DATASET 만들기 \n",
    "\n",
    "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train')\n",
    "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2f44ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET 로딩하기\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "923ffa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 1000\n",
    "\n",
    "NUM_FEATURES = 23\n",
    "HIDDEN_SIZE = 4096\n",
    "EARLY_STOPPING_PATIENCE = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4b03eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_features, num_targets=3, hidden_size=2048):\n",
    "        super(CNN1D, self).__init__()\n",
    "        cha_1 = 256\n",
    "        cha_2 = 512\n",
    "        cha_3 = 512\n",
    "\n",
    "        cha_1_reshape = int(hidden_size/cha_1)\n",
    "        cha_po_1 = int(cha_1_reshape/2)\n",
    "        cha_po_2 = int(cha_po_1/2) * cha_3\n",
    "\n",
    "        self.cha_1 = cha_1\n",
    "        self.cha_2 = cha_2\n",
    "        self.cha_3 = cha_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "\n",
    "        # Dense\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "        # CNN 1\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "        self.dropout_c1 = nn.Dropout(0.1)\n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
    "\n",
    "        # Avg-Pool\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "        # CNN 2_0\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2 = nn.Dropout(0.1)\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "        # CNN 2_1\n",
    "        self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_1 = nn.Dropout(0.3)\n",
    "        self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "        # CNN 2_2\n",
    "        self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_2 = nn.Dropout(0.2)\n",
    "        self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n",
    "\n",
    "        # Max-Pool\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Flatten\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        # Dense 1\n",
    "        self.batch_norm3_1 = nn.BatchNorm1d(cha_po_2)\n",
    "        self.dropout3_1 = nn.Dropout(0.2)\n",
    "        self.dense3_1 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "        # Dense 2\n",
    "        self.batch_norm3_2 = nn.BatchNorm1d(cha_po_2)\n",
    "        self.dropout3_2 = nn.Dropout(0.2)\n",
    "        self.dense3_2 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "        # Dense 3\n",
    "        self.batch_norm3_3 = nn.BatchNorm1d(cha_po_2)\n",
    "        self.dropout3_3 = nn.Dropout(0.2)\n",
    "        self.dense3_3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Dense\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "        # Reshape\n",
    "        x = x.reshape(x.shape[0],self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        # CNN 1\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = self.dropout_c1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        # Avg-Pool\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        # CNN 2_0\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x_s = x\n",
    "\n",
    "        # CNN 2_1\n",
    "        x = self.batch_norm_c2_1(x)\n",
    "        x = self.dropout_c2_1(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "\n",
    "        # CNN 2_2\n",
    "        x = self.batch_norm_c2_2(x)\n",
    "        x = self.dropout_c2_2(x)\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x =  x * x_s\n",
    "\n",
    "        # Max-Pool\n",
    "        x = self.max_po_c2(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = self.flt(x)\n",
    "\n",
    "        # Dense 1\n",
    "        x_1 = self.batch_norm3_1(x)\n",
    "        x_1 = self.dropout3_1(x_1)\n",
    "        x_1 = self.dense3_1(x_1)\n",
    "\n",
    "        # Dense 2\n",
    "        x_2 = self.batch_norm3_2(x)\n",
    "        x_2 = self.dropout3_2(x_2)\n",
    "        x_2 = self.dense3_2(x_2)\n",
    "\n",
    "        # Dense 3\n",
    "        x_3 = self.batch_norm3_3(x)\n",
    "        x_3 = self.dropout3_3(x_3)\n",
    "        x_3 = self.dense3_3(x_3)\n",
    "\n",
    "        return x_1, x_2, x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d211432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성하기\n",
    "model = CNN1D(num_features=NUM_FEATURES, hidden_size=HIDDEN_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e2b2b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = 0.00001\n",
    "\n",
    "# Set optimizer, scheduler, metric function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "\n",
    "# Set loss functions for multihead classifier\n",
    "loss_fn_1 = nn.CrossEntropyLoss()\n",
    "loss_fn_2 = nn.CrossEntropyLoss()\n",
    "loss_fn_3 = nn.CrossEntropyLoss()\n",
    "loss_fn = [loss_fn_1, loss_fn_2, loss_fn_3]\n",
    "\n",
    "# Set metrics\n",
    "metric_fn = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bea21bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch별 절차를 정의하는 Trainer class\n",
    "\n",
    "class CustomTrainer():\n",
    "\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \n",
    "    Attributes:\n",
    "        model (`model`)\n",
    "        device (str)\n",
    "        loss_fn (Callable)\n",
    "        metric_fn (Callable)\n",
    "        optimizer (`optimizer`)\n",
    "        scheduler (`scheduler`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, device, loss_fn, metric_fn, optimizer=None, scheduler=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        self.loss_fn_1 = loss_fn[0]\n",
    "        self.loss_fn_2 = loss_fn[1]\n",
    "        self.loss_fn_3 = loss_fn[2]\n",
    "\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        # History - loss\n",
    "        self.train_batch_loss_mean_list = list()\n",
    "        self.train_batch_score_list = list()\n",
    "\n",
    "        self.validation_batch_loss_mean_list = list()\n",
    "        self.validation_batch_score_list = list()\n",
    "\n",
    "        # History - predict\n",
    "        self.train_target_pred_1_list = list()\n",
    "        self.train_target_1_list = list()\n",
    "        self.train_target_pred_2_list = list()\n",
    "        self.train_target_2_list = list()\n",
    "        self.train_target_pred_3_list = list()\n",
    "        self.train_target_3_list = list()\n",
    "\n",
    "        self.validation_target_pred_1_list = list()\n",
    "        self.validation_target_1_list = list()\n",
    "        self.validation_target_pred_2_list = list()\n",
    "        self.validation_target_2_list = list()\n",
    "        self.validation_target_pred_3_list = list()\n",
    "        self.validation_target_3_list = list()\n",
    "\n",
    "        # Output\n",
    "        self.train_loss_mean = 0\n",
    "        self.train_loss_sum = 0\n",
    "        self.train_score = 0\n",
    "\n",
    "        self.validation_loss_mean = 0\n",
    "        self.validation_loss_sum = 0\n",
    "        self.validation_score = 0\n",
    "\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index=0, verbose=False):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "\n",
    "        Args:\n",
    "            dataloader (`dataloader`)\n",
    "            epoch_index (int)\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        for batch_index, (feature, target_1, target_2, target_3) in enumerate(dataloader):\n",
    "            feature, target_1, target_2, target_3 = feature.to(self.device), target_1.to(self.device), target_2.to(self.device), target_3.to(self.device)\n",
    "\n",
    "            ## Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문에 \n",
    "            ## 항상 backpropagation을 하기전에 gradients를 zero로 만들어주고 시작을 해야합니다.\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Summing up losses for Multihead\n",
    "            target_pred_1, target_pred_2, target_pred_3 = self.model(feature)\n",
    "            batch_loss_1_mean = self.loss_fn_1(target_pred_1, target_1)\n",
    "            batch_loss_2_mean = self.loss_fn_2(target_pred_2, target_2)\n",
    "            batch_loss_3_mean = self.loss_fn_3(target_pred_3, target_3)\n",
    "            batch_loss_mean = batch_loss_1_mean + batch_loss_2_mean + batch_loss_3_mean\n",
    "            batch_loss_sum = batch_loss_mean.item() * dataloader.batch_size\n",
    "            self.train_batch_loss_mean_list.append(batch_loss_mean.item())\n",
    "            self.train_loss_sum += batch_loss_sum\n",
    "\n",
    "            # Calculate multiple batch scores for Multihead\n",
    "            target_1_list = target_1.cpu().tolist()\n",
    "            target_2_list = target_2.cpu().tolist()\n",
    "            target_3_list = target_3.cpu().tolist()\n",
    "            target_pred_1_list = torch.argmax(target_pred_1, dim=1).cpu().tolist()\n",
    "            target_pred_2_list = torch.argmax(target_pred_2, dim=1).cpu().tolist()\n",
    "            target_pred_3_list = torch.argmax(target_pred_3, dim=1).cpu().tolist()\n",
    "            batch_score_1 = self.metric_fn(target_1_list, target_pred_1_list, average='macro')\n",
    "            batch_score_2 = self.metric_fn(target_2_list, target_pred_2_list, average='macro')\n",
    "            batch_score_3 = self.metric_fn(target_3_list, target_pred_3_list, average='macro')\n",
    "            self.train_batch_score_list.append(sum([batch_score_1, batch_score_2, batch_score_3]) / 3)\n",
    "\n",
    "            # History - predict\n",
    "            self.train_target_1_list.extend(target_1_list)\n",
    "            self.train_target_pred_1_list.extend(target_pred_1_list)\n",
    "            self.train_target_2_list.extend(target_2_list)\n",
    "            self.train_target_pred_2_list.extend(target_pred_2_list)\n",
    "            self.train_target_3_list.extend(target_3_list)\n",
    "            self.train_target_pred_3_list.extend(target_pred_3_list)\n",
    "\n",
    "            batch_loss_mean.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "        self.train_loss_mean = self.train_loss_sum / len(dataloader)\n",
    "\n",
    "        train_score_1 = self.metric_fn(self.train_target_1_list, self.train_target_pred_1_list, average='macro')\n",
    "        train_score_2 = self.metric_fn(self.train_target_2_list, self.train_target_pred_2_list, average='macro')\n",
    "        train_score_3 = self.metric_fn(self.train_target_3_list, self.train_target_pred_3_list, average='macro')\n",
    "        self.train_score = sum([train_score_1, train_score_2, train_score_3]) / 3\n",
    "\n",
    "        msg = f'Epoch {epoch_index}, Train, Mean loss: {self.train_loss_mean}, Score: {self.train_score}'\n",
    "\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index=0):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "\n",
    "        Args:\n",
    "            dataloader (`dataloader`)\n",
    "            epoch_index (int)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (feature, target_1, target_2, target_3) in enumerate(dataloader):\n",
    "                feature, target_1, target_2, target_3 = feature.to(self.device), target_1.to(self.device), target_2.to(self.device), target_3.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Summing up losses for Multihead\n",
    "                target_pred_1, target_pred_2, target_pred_3 = self.model(feature)\n",
    "                batch_loss_1_mean = self.loss_fn_1(target_pred_1, target_1)\n",
    "                batch_loss_2_mean = self.loss_fn_2(target_pred_2, target_2)\n",
    "                batch_loss_3_mean = self.loss_fn_3(target_pred_3, target_3)\n",
    "                batch_loss_mean = batch_loss_1_mean + batch_loss_2_mean + batch_loss_3_mean\n",
    "                batch_loss_sum = batch_loss_mean.item() * dataloader.batch_size\n",
    "                self.validation_batch_loss_mean_list.append(batch_loss_mean.item())\n",
    "                self.validation_loss_sum += batch_loss_sum\n",
    "\n",
    "                # Calculate multiple batch scores for Multihead\n",
    "                target_1_list = target_1.cpu().tolist()\n",
    "                target_2_list = target_2.cpu().tolist()\n",
    "                target_3_list = target_3.cpu().tolist()\n",
    "                target_pred_1_list = torch.argmax(target_pred_1, dim=1).cpu().tolist()\n",
    "                target_pred_2_list = torch.argmax(target_pred_2, dim=1).cpu().tolist()\n",
    "                target_pred_3_list = torch.argmax(target_pred_3, dim=1).cpu().tolist()\n",
    "                batch_score_1 = self.metric_fn(target_1_list, target_pred_1_list, average='macro')\n",
    "                batch_score_2 = self.metric_fn(target_2_list, target_pred_2_list, average='macro')\n",
    "                batch_score_3 = self.metric_fn(target_3_list, target_pred_3_list, average='macro')\n",
    "                self.validation_batch_score_list.append(sum([batch_score_1, batch_score_2, batch_score_3]) / 3)\n",
    "\n",
    "                # History - predict\n",
    "                self.validation_target_1_list.extend(target_1_list)\n",
    "                self.validation_target_pred_1_list.extend(target_pred_1_list)\n",
    "                self.validation_target_2_list.extend(target_2_list)\n",
    "                self.validation_target_pred_2_list.extend(target_pred_2_list)\n",
    "                self.validation_target_3_list.extend(target_3_list)\n",
    "                self.validation_target_pred_3_list.extend(target_pred_3_list)\n",
    "\n",
    "            self.validation_loss_mean = self.validation_loss_sum / len(dataloader)\n",
    "\n",
    "            validation_score_1 = self.metric_fn(self.validation_target_1_list, self.validation_target_pred_1_list, average='macro')\n",
    "            validation_score_2 = self.metric_fn(self.validation_target_2_list, self.validation_target_pred_2_list, average='macro')\n",
    "            validation_score_3 = self.metric_fn(self.validation_target_3_list, self.validation_target_pred_3_list, average='macro')\n",
    "            self.validation_score = sum([validation_score_1, validation_score_2, validation_score_3]) / 3\n",
    "\n",
    "            msg = f'Epoch {epoch_index}, Validation, Mean loss: {self.validation_loss_mean}, Score: {self.validation_score}'\n",
    "\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\" 한 epoch 종료 후 history 초기화\n",
    "            Examples:\n",
    "                >>for epoch_index in tqdm(range(EPOCH)):\n",
    "                >>    trainer.train_epoch(dataloader=train_dataloader, epoch_index=epoch_index, verbose=False)\n",
    "                >>    trainer.validate_epoch(dataloader=validation_dataloader, epoch_index=epoch_index, verbose=False)\n",
    "                >>    trainer.clear_history()\n",
    "        \"\"\"\n",
    "\n",
    "        # History - loss\n",
    "        self.train_batch_loss_mean_list = list()\n",
    "        self.train_batch_score_list = list()\n",
    "\n",
    "        self.validation_batch_loss_mean_list = list()\n",
    "        self.validation_batch_score_list = list()\n",
    "\n",
    "        # History - predict\n",
    "        self.train_target_pred_1_list = list()\n",
    "        self.train_target_1_list = list()\n",
    "        self.train_target_pred_2_list = list()\n",
    "        self.train_target_2_list = list()\n",
    "        self.train_target_pred_3_list = list()\n",
    "        self.train_target_3_list = list()\n",
    "\n",
    "        self.validation_target_pred_1_list = list()\n",
    "        self.validation_target_1_list = list()\n",
    "        self.validation_target_pred_2_list = list()\n",
    "        self.validation_target_2_list = list()\n",
    "        self.validation_target_pred_3_list = list()\n",
    "        self.validation_target_3_list = list()\n",
    "\n",
    "        # Output\n",
    "        self.train_loss_mean = 0\n",
    "        self.train_loss_sum = 0\n",
    "        self.train_score = 0\n",
    "\n",
    "        self.validation_loss_mean = 0\n",
    "        self.validation_loss_sum = 0\n",
    "        self.validation_score = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "88b69e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        \"\"\" 초기화\n",
    "\n",
    "        Args:\n",
    "            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "            weight_path (str): weight 저장경로\n",
    "            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\n",
    "\n",
    "        Args:\n",
    "            loss (float):\n",
    "\n",
    "        Examples:\n",
    "            \n",
    "        Note:\n",
    "            \n",
    "        \"\"\"  \n",
    "\n",
    "        if self.min_loss == np.Inf:\n",
    "            #첫 에폭\n",
    "            self.min_loss = loss\n",
    "            # self.save_checkpoint(loss=loss, model=model)\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            # loss가 줄지 않음 -> patience_counter 1 증가\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopper, Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "\n",
    "\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            # loss가 줄어듬 -> min_loss 갱신\n",
    "            self.save_model = True\n",
    "            msg = f\"Early stopper, Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a75cc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 셋팅하기\n",
    "trainer = CustomTrainer(model, device, loss_fn, metric_fn, optimizer, scheduler)\n",
    "\n",
    "# Earlystopper 셋팅하기\n",
    "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ded14b",
   "metadata": {},
   "source": [
    "## 학습\n",
    "각 에폭별 수행할 절차를 trainer 클래스의 train_epoch, validation_epoch을 이용해 정의합니다.  \n",
    "EPOCHS만큼, 혹은 early_stop이 될 때까지 에폭을 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4ca8573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 에폭 별로 train_epoch, valid_epoch 실헹힘\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_index \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(EPOCHS)):\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mvalidate_epoch(validation_dataloader, epoch_index\u001b[38;5;241m=\u001b[39mepoch_index)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# early_stopping check\u001b[39;00m\n",
      "Input \u001b[1;32mIn [75]\u001b[0m, in \u001b[0;36mCustomTrainer.train_epoch\u001b[1;34m(self, dataloader, epoch_index, verbose)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\" 한 epoch에서 수행되는 학습 절차\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    dataloader (`dataloader`)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    epoch_index (int)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index, (feature, target_1, target_2, target_3) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     73\u001b[0m     feature, target_1, target_2, target_3 \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), target_1\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), target_2\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), target_3\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m## Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문에 \u001b[39;00m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m## 항상 backpropagation을 하기전에 gradients를 zero로 만들어주고 시작을 해야합니다.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml02\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml02\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml02\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml02\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:183\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    181\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_PATH, 'best.pt')\n",
    "start = time.time()\n",
    "\n",
    "criterion = 0\n",
    "\n",
    "# 에폭 별로 train_epoch, valid_epoch 실헹힘\n",
    "for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "    trainer.train_epoch(train_dataloader, epoch_index=epoch_index)\n",
    "    trainer.validate_epoch(validation_dataloader, epoch_index=epoch_index)\n",
    "   \n",
    "    # early_stopping check\n",
    "    early_stopper.check_early_stopping(loss=trainer.validation_loss_mean)\n",
    "        \n",
    "    if early_stopper.stop:\n",
    "        print('Early stopped')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    if trainer.validation_score > criterion:\n",
    "        # 모델이 개선됨 -> 검증 점수와 weight 갱신\n",
    "        criterion = trainer.validation_score\n",
    "        torch.save(model.state_dict(), MODEL_DIR)\n",
    "        check_point = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(check_point, os.path.join(ROOT_PATH, 'best.pt'))\n",
    "\n",
    "\n",
    "    trainer.clear_history()\n",
    "\n",
    "print(\"Model saved: best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ddf31c",
   "metadata": {},
   "source": [
    "## 추론\n",
    "테스트 데이터의 타겟 변수를 `sample_submission.csv` 양식에 맞춰 저장한 파일을 플랫폼을 통해 제출하면 추론 점수를 확인할 수 있습니다.  \n",
    "\n",
    "\"역률\", \"전류고조파\", \"전압고조파\" 컬럼 값을 여러분의 모델의 추론 결과로 채워 제출 파일을 만듭니다 (현재는 모두 아래 보시는 바와 같이 동일한 값으로 채워져 있습니다). INDEX 값을 기준으로 채점을 진행하는 점 유의해주시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4cfa8c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>누적전력량</th>\n",
       "      <th>유효전력평균</th>\n",
       "      <th>무효전력평균</th>\n",
       "      <th>주파수</th>\n",
       "      <th>전류평균</th>\n",
       "      <th>상전압평균</th>\n",
       "      <th>선간전압평균</th>\n",
       "      <th>온도</th>\n",
       "      <th>R상유효전력</th>\n",
       "      <th>...</th>\n",
       "      <th>S상전압</th>\n",
       "      <th>S상선간전압</th>\n",
       "      <th>T상유효전력</th>\n",
       "      <th>T상무효전력</th>\n",
       "      <th>T상전류</th>\n",
       "      <th>T상전압</th>\n",
       "      <th>T상선간전압</th>\n",
       "      <th>label_역률평균</th>\n",
       "      <th>label_전류고조파평균</th>\n",
       "      <th>label_전압고조파평균</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2248930.5</td>\n",
       "      <td>28963.0</td>\n",
       "      <td>20237.0</td>\n",
       "      <td>59.854076</td>\n",
       "      <td>45.197918</td>\n",
       "      <td>259.916656</td>\n",
       "      <td>449.916656</td>\n",
       "      <td>47.500</td>\n",
       "      <td>8663.0</td>\n",
       "      <td>...</td>\n",
       "      <td>260.75</td>\n",
       "      <td>452.75</td>\n",
       "      <td>10417.0</td>\n",
       "      <td>7304.0</td>\n",
       "      <td>48.71875</td>\n",
       "      <td>260.75</td>\n",
       "      <td>448.75</td>\n",
       "      <td>정상</td>\n",
       "      <td>경고</td>\n",
       "      <td>주의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.68750</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>경고</td>\n",
       "      <td>정상</td>\n",
       "      <td>정상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5375707.0</td>\n",
       "      <td>35244.0</td>\n",
       "      <td>19826.0</td>\n",
       "      <td>59.975650</td>\n",
       "      <td>107.385414</td>\n",
       "      <td>125.416664</td>\n",
       "      <td>216.750000</td>\n",
       "      <td>18.750</td>\n",
       "      <td>11988.0</td>\n",
       "      <td>...</td>\n",
       "      <td>125.25</td>\n",
       "      <td>216.25</td>\n",
       "      <td>12236.0</td>\n",
       "      <td>6170.0</td>\n",
       "      <td>109.56250</td>\n",
       "      <td>125.00</td>\n",
       "      <td>216.25</td>\n",
       "      <td>정상</td>\n",
       "      <td>정상</td>\n",
       "      <td>주의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17781200.0</td>\n",
       "      <td>77056.0</td>\n",
       "      <td>39520.0</td>\n",
       "      <td>59.863000</td>\n",
       "      <td>244.854000</td>\n",
       "      <td>118.083000</td>\n",
       "      <td>205.333000</td>\n",
       "      <td>23.125</td>\n",
       "      <td>25796.0</td>\n",
       "      <td>...</td>\n",
       "      <td>118.75</td>\n",
       "      <td>118.75</td>\n",
       "      <td>24992.0</td>\n",
       "      <td>13704.0</td>\n",
       "      <td>242.18800</td>\n",
       "      <td>118.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>정상</td>\n",
       "      <td>정상</td>\n",
       "      <td>경고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10143988.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.798140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.750000</td>\n",
       "      <td>231.500000</td>\n",
       "      <td>26.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>134.50</td>\n",
       "      <td>231.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>133.00</td>\n",
       "      <td>230.25</td>\n",
       "      <td>경고</td>\n",
       "      <td>정상</td>\n",
       "      <td>정상</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       누적전력량   유효전력평균   무효전력평균        주파수        전류평균       상전압평균  \\\n",
       "0      0   2248930.5  28963.0  20237.0  59.854076   45.197918  259.916656   \n",
       "1      1         0.0      0.0      0.0   0.000000  101.312500    0.000000   \n",
       "2      2   5375707.0  35244.0  19826.0  59.975650  107.385414  125.416664   \n",
       "3      3  17781200.0  77056.0  39520.0  59.863000  244.854000  118.083000   \n",
       "4      4  10143988.0      0.0      0.0  59.798140    0.000000  133.750000   \n",
       "\n",
       "       선간전압평균      온도   R상유효전력  ...    S상전압  S상선간전압   T상유효전력   T상무효전력  \\\n",
       "0  449.916656  47.500   8663.0  ...  260.75  452.75  10417.0   7304.0   \n",
       "1    0.000000  24.375      0.0  ...    0.00    0.00      0.0      0.0   \n",
       "2  216.750000  18.750  11988.0  ...  125.25  216.25  12236.0   6170.0   \n",
       "3  205.333000  23.125  25796.0  ...  118.75  118.75  24992.0  13704.0   \n",
       "4  231.500000  26.875      0.0  ...  134.50  231.50      0.0      0.0   \n",
       "\n",
       "        T상전류    T상전압  T상선간전압  label_역률평균  label_전류고조파평균  label_전압고조파평균  \n",
       "0   48.71875  260.75  448.75          정상             경고             주의  \n",
       "1  100.68750    0.00    0.00          경고             정상             정상  \n",
       "2  109.56250  125.00  216.25          정상             정상             주의  \n",
       "3  242.18800  118.00  118.00          정상             정상             경고  \n",
       "4    0.00000  133.00  230.25          경고             정상             정상  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e36d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset..\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 로드\n",
    "test_dataset = CustomDataset(data_dir=DATA_DIR, mode='test')\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d0fa3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313267"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee55b086",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m HIDDEN_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN1D(NUM_FEATURES, hidden_size\u001b[38;5;241m=\u001b[39mHIDDEN_SIZE)\n\u001b[1;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_DIR\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 추론\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml02\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml02\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml02\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best.pt'"
     ]
    }
   ],
   "source": [
    "\"\"\" 이전에 학습한 모델 weight파일을 불러 추론하려면 아래 주석을 풀고 실행\n",
    "    학습 진행 후 바로 추론하는 경우 학습 과정의 model 사용 (주석 풀지 않고 실행) \"\"\"\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_PATH, 'best.pt')\n",
    "\n",
    "# 모델 사이즈와 맞춰주기 위해 재설정\n",
    "HIDDEN_SIZE = 4096\n",
    "\n",
    "model = CNN1D(NUM_FEATURES, hidden_size=HIDDEN_SIZE)\n",
    "model.load_state_dict(torch.load(MODEL_DIR)['model'])\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "\n",
    "# 추론 결과를 pred 리스트로 저장\n",
    "pred1 = []\n",
    "pred2 = []\n",
    "pred3 = []\n",
    "with torch.no_grad():\n",
    "    ##train_epoch 참고\n",
    "    for batch_index, (feature) in enumerate(test_dataloader):\n",
    "        output1, output2, output3 = model(feature)\n",
    "\n",
    "        pred1.extend(torch.argmax(output1, dim=1).cpu().tolist())\n",
    "        pred2.extend(torch.argmax(output2, dim=1).cpu().tolist())\n",
    "        pred3.extend(torch.argmax(output3, dim=1).cpu().tolist())\n",
    "       \n",
    "        # 진행과정 출력\n",
    "        if batch_index % 50 == 0:\n",
    "            print(f'Prediction: {batch_index}/{len(test_dataloader)} completed')\n",
    "    print(\"Prediction all completed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8c98223",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 인코딩 값으로 나온 타겟 변수를 디코딩\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pred1 \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mlabel_decoder(\u001b[43mpred1\u001b[49m)\n\u001b[0;32m      3\u001b[0m pred2 \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mlabel_decoder(pred2)\n\u001b[0;32m      4\u001b[0m pred3 \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mlabel_decoder(pred3)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 인코딩 값으로 나온 타겟 변수를 디코딩\n",
    "pred1 = test_dataset.label_decoder(pred1)\n",
    "pred2 = test_dataset.label_decoder(pred2)\n",
    "pred3 = test_dataset.label_decoder(pred3)\n",
    "\n",
    "print('decode COMPLETED')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29897d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 확인\n",
    "submit['label_역률평균'] = pred1\n",
    "submit['label_전류고조파평균'] = pred2\n",
    "submit['label_전압고조파평균'] = pred3\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 제작\n",
    "submit.to_csv(os.path.join(ROOT_PATH,'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2bbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decbc22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e22c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
